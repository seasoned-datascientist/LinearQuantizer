<img width="747" alt="Screenshot 2025-03-05 at 09 44 54" src="https://github.com/user-attachments/assets/615e1998-0c6d-4045-b97e-cd56310caf89" />

LLMs can be massive and hard to run on regular devices, so I built my own linear quantizer to make them more efficient!

Here’s what I did:

✅ Developed a custom per-channel quantization tool

✅ Tested it on dummy models to refine the technique


✅ Applied it to open-source models to see real-world results



With per-channel quantization, we’re able to compress models even more effectively without losing accuracy.
This means AI models can now run on smartphones, PCs, and edge devices with greater efficiency. 
Check out my code below—this is the real deal, and it goes beyond the basics!


